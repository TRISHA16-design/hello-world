{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMGJG7rULfGTUlkxaH8R2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TRISHA16-design/hello-world/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "pYHkSruupNoG",
        "outputId": "f21f83a5-8cec-4ffa-efe3-6d24bf3500ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your two CSV files:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d12a24fa-396a-4ee2-87d3-2cf10c7e05c4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d12a24fa-396a-4ee2-87d3-2cf10c7e05c4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving rotten_tomatoes_movie_reviews.csv to rotten_tomatoes_movie_reviews.csv\n",
            "Saving rotten_tomatoes_movies.csv to rotten_tomatoes_movies.csv\n",
            "\n",
            "Successfully uploaded 2 file(s):\n",
            "- rotten_tomatoes_movie_reviews.csv\n",
            "  Successfully loaded 'rotten_tomatoes_movie_reviews.csv' into a DataFrame.\n",
            "- rotten_tomatoes_movies.csv\n",
            "  Successfully loaded 'rotten_tomatoes_movies.csv' into a DataFrame.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "print(\"Please upload your two CSV files:\")\n",
        "# This will open a dialog. Select both your CSV files.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if files were uploaded\n",
        "if not uploaded:\n",
        "    print(\"No files were uploaded.\")\n",
        "else:\n",
        "    print(f\"\\nSuccessfully uploaded {len(uploaded)} file(s):\")\n",
        "\n",
        "    # Create a dictionary to store your dataframes\n",
        "    dataframes = {}\n",
        "\n",
        "    for file_name, file_content in uploaded.items():\n",
        "        print(f\"- {file_name}\")\n",
        "        if file_name.lower().endswith('.csv'):\n",
        "            try:\n",
        "                # Read the CSV into a pandas DataFrame\n",
        "                df = pd.read_csv(io.BytesIO(file_content))\n",
        "                dataframes[file_name] = df\n",
        "                print(f\"  Successfully loaded '{file_name}' into a DataFrame.\")\n",
        "                # Optionally display the head of each dataframe\n",
        "                # print(\"  First 5 rows:\")\n",
        "                # display(df.head())\n",
        "            except Exception as e:\n",
        "                print(f\"  Error reading CSV '{file_name}': {e}\")\n",
        "        else:\n",
        "            print(f\"  '{file_name}' is not a CSV file and will be ignored for DataFrame creation.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and Data Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re # For basic text cleaning\n",
        "\n",
        "# IMPORTANT: Upload your 'rotten_tomatoes_movies.csv' and 'rotten_tomatoes_movie_reviews.csv'\n",
        "# files to your Colab environment before running this cell.\n",
        "# You can do this by clicking on the \"Files\" icon on the left sidebar, then \"Upload\".\n",
        "\n",
        "try:\n",
        "    movies_df_raw = pd.read_csv('rotten_tomatoes_movies.csv')\n",
        "    reviews_df_raw = pd.read_csv('rotten_tomatoes_movie_reviews.csv')\n",
        "    print(\"Datasets loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Make sure 'rotten_tomatoes_movies.csv' and 'rotten_tomatoes_movie_reviews.csv' are uploaded to Colab.\")\n",
        "    print(\"Creating dummy DataFrames for demonstration purposes. Functionality will be limited.\")\n",
        "    # Create dummy dataframes to allow the rest of the script to run without errors for demonstration\n",
        "    movies_df_raw = pd.DataFrame({\n",
        "        'id': ['m1', 'm2', 'm3'],\n",
        "        'title': ['Dummy Movie 1', 'Dummy Movie 2', 'Another Dummy Film'],\n",
        "        'audienceScore': [60, 70, 80],\n",
        "        'tomatoMeter': [65, 75, 85],\n",
        "        'rating': ['PG', 'PG-13', 'R'],\n",
        "        'ratingContents': ['Mild action', 'Some violence', 'Strong language'],\n",
        "        'genre': ['Action|Adventure', 'Comedy', 'Drama|Thriller'],\n",
        "        'director': ['Director A', 'Director B', 'Director C'],\n",
        "        'writer': ['Writer X', 'Writer Y', 'Writer Z']\n",
        "    })\n",
        "    reviews_df_raw = pd.DataFrame({\n",
        "        'id': ['m1', 'm1', 'm2', 'm3'],\n",
        "        'reviewText': ['Good fun', 'Enjoyable for family', 'Very funny', 'Gripping story'],\n",
        "        'scoreSentiment': ['POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE']\n",
        "    })\n",
        "\n",
        "print(\"\\n--- Raw Movies DataFrame Head ---\")\n",
        "print(movies_df_raw.head())\n",
        "print(f\"\\nRaw Movies DataFrame Shape: {movies_df_raw.shape}\")\n",
        "\n",
        "print(\"\\n--- Raw Reviews DataFrame Head ---\")\n",
        "print(reviews_df_raw.head())\n",
        "print(f\"\\nRaw Reviews DataFrame Shape: {reviews_df_raw.shape}\")\n",
        "\n",
        "# Make copies to work with, preserving the raw loaded data\n",
        "movies_df = movies_df_raw.copy()\n",
        "reviews_df = reviews_df_raw.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6JNiGL_cFMQ",
        "outputId": "532b7197-0ffb-4990-b231-358c306b7e08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully!\n",
            "\n",
            "--- Raw Movies DataFrame Head ---\n",
            "                     id                title  audienceScore  tomatoMeter  \\\n",
            "0    space-zombie-bingo  Space Zombie Bingo!           50.0          NaN   \n",
            "1       the_green_grass      The Green Grass            NaN          NaN   \n",
            "2             love_lies           Love, Lies           43.0          NaN   \n",
            "3  the_sore_losers_1997          Sore Losers           60.0          NaN   \n",
            "4  dinosaur_island_2002      Dinosaur Island           70.0          NaN   \n",
            "\n",
            "  rating ratingContents releaseDateTheaters releaseDateStreaming  \\\n",
            "0    NaN            NaN                 NaN           2018-08-25   \n",
            "1    NaN            NaN                 NaN           2020-02-11   \n",
            "2    NaN            NaN                 NaN                  NaN   \n",
            "3    NaN            NaN                 NaN           2020-10-23   \n",
            "4    NaN            NaN                 NaN           2017-03-27   \n",
            "\n",
            "   runtimeMinutes                          genre originalLanguage  \\\n",
            "0            75.0         Comedy, Horror, Sci-fi          English   \n",
            "1           114.0                          Drama          English   \n",
            "2           120.0                          Drama           Korean   \n",
            "3            90.0     Action, Mystery & thriller          English   \n",
            "4            80.0  Fantasy, Adventure, Animation          English   \n",
            "\n",
            "                        director                                  writer  \\\n",
            "0                  George Ormrod              George Ormrod,John Sabotta   \n",
            "1                Tiffany Edwards                         Tiffany Edwards   \n",
            "2  Park Heung-Sik,Heung-Sik Park  Ha Young-Joon,Jeon Yun-su,Song Hye-jin   \n",
            "3          John Michael McCarthy                   John Michael McCarthy   \n",
            "4                  Will Meugniot                                John Loy   \n",
            "\n",
            "  boxOffice distributor soundMix  \n",
            "0       NaN         NaN      NaN  \n",
            "1       NaN         NaN      NaN  \n",
            "2       NaN         NaN      NaN  \n",
            "3       NaN         NaN      NaN  \n",
            "4       NaN         NaN      NaN  \n",
            "\n",
            "Raw Movies DataFrame Shape: (143258, 16)\n",
            "\n",
            "--- Raw Reviews DataFrame Head ---\n",
            "                                  id  reviewId creationDate       criticName  \\\n",
            "0                            beavers   1145982   2003-05-23  Ivan M. Lincoln   \n",
            "1                         blood_mask   1636744   2007-06-02    The Foywonder   \n",
            "2  city_hunter_shinjuku_private_eyes   2590987   2019-05-28     Reuben Baron   \n",
            "3  city_hunter_shinjuku_private_eyes   2558908   2019-02-14      Matt Schley   \n",
            "4                 dangerous_men_2015   2504681   2018-08-29        Pat Padua   \n",
            "\n",
            "   isTopCritic originalScore reviewState                 publicatioName  \\\n",
            "0        False         3.5/4       fresh  Deseret News (Salt Lake City)   \n",
            "1        False           1/5      rotten                  Dread Central   \n",
            "2        False           NaN       fresh                            CBR   \n",
            "3        False         2.5/5      rotten                    Japan Times   \n",
            "4        False           NaN       fresh                          DCist   \n",
            "\n",
            "                                          reviewText scoreSentiment  \\\n",
            "0  Timed to be just long enough for most youngste...       POSITIVE   \n",
            "1  It doesn't matter if a movie costs 300 million...       NEGATIVE   \n",
            "2  The choreography is so precise and lifelike at...       POSITIVE   \n",
            "3  The film's out-of-touch attempts at humor may ...       NEGATIVE   \n",
            "4  Its clumsy determination is endearing and some...       POSITIVE   \n",
            "\n",
            "                                           reviewUrl  \n",
            "0  http://www.deseretnews.com/article/700003233/B...  \n",
            "1  http://www.dreadcentral.com/index.php?name=Rev...  \n",
            "2  https://www.cbr.com/city-hunter-shinjuku-priva...  \n",
            "3  https://www.japantimes.co.jp/culture/2019/02/0...  \n",
            "4  http://dcist.com/2015/11/out_of_frame_dangerou...  \n",
            "\n",
            "Raw Reviews DataFrame Shape: (1444963, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Preprocess Movies Data\n",
        "print(\"\\n--- Preprocessing Movies Data ---\")\n",
        "\n",
        "# Select relevant columns from movies_df\n",
        "# Added 'id' to ensure it's available for merging if not already first col\n",
        "movies_cols = ['id', 'title', 'audienceScore', 'tomatoMeter', 'rating', 'ratingContents', 'genre', 'director', 'writer']\n",
        "# Ensure all selected columns exist, otherwise, only use existing ones\n",
        "movies_cols_exist = [col for col in movies_cols if col in movies_df.columns]\n",
        "movies_df = movies_df[movies_cols_exist]\n",
        "\n",
        "\n",
        "# Handle missing values for key textual features\n",
        "for col in ['genre', 'director', 'ratingContents', 'writer', 'title']:\n",
        "    if col in movies_df.columns:\n",
        "        movies_df[col] = movies_df[col].fillna('')\n",
        "        if col == 'title': # Specifically ensure 'title' is not empty for lookup\n",
        "             movies_df[col] = movies_df[col].apply(lambda x: 'Unknown Title' if x == '' else x)\n",
        "\n",
        "\n",
        "# Handle missing numerical scores (optional: fill with mean/median, or 0)\n",
        "for col in ['audienceScore', 'tomatoMeter']:\n",
        "    if col in movies_df.columns:\n",
        "        movies_df[col] = movies_df[col].fillna(0)\n",
        "\n",
        "print(\"\\n--- Processed Movies DataFrame Head ---\")\n",
        "print(movies_df.head())\n",
        "print(f\"\\nProcessed Movies DataFrame Shape: {movies_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ9_6VP5cU4v",
        "outputId": "1b2cceaf-9744-41b4-f1e0-5885c52caf55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Preprocessing Movies Data ---\n",
            "\n",
            "--- Processed Movies DataFrame Head ---\n",
            "                     id                title  audienceScore  tomatoMeter  \\\n",
            "0    space-zombie-bingo  Space Zombie Bingo!           50.0          0.0   \n",
            "1       the_green_grass      The Green Grass            0.0          0.0   \n",
            "2             love_lies           Love, Lies           43.0          0.0   \n",
            "3  the_sore_losers_1997          Sore Losers           60.0          0.0   \n",
            "4  dinosaur_island_2002      Dinosaur Island           70.0          0.0   \n",
            "\n",
            "  rating ratingContents                          genre  \\\n",
            "0    NaN                        Comedy, Horror, Sci-fi   \n",
            "1    NaN                                         Drama   \n",
            "2    NaN                                         Drama   \n",
            "3    NaN                    Action, Mystery & thriller   \n",
            "4    NaN                 Fantasy, Adventure, Animation   \n",
            "\n",
            "                        director                                  writer  \n",
            "0                  George Ormrod              George Ormrod,John Sabotta  \n",
            "1                Tiffany Edwards                         Tiffany Edwards  \n",
            "2  Park Heung-Sik,Heung-Sik Park  Ha Young-Joon,Jeon Yun-su,Song Hye-jin  \n",
            "3          John Michael McCarthy                   John Michael McCarthy  \n",
            "4                  Will Meugniot                                John Loy  \n",
            "\n",
            "Processed Movies DataFrame Shape: (143258, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Preprocess and Aggregate Reviews Data\n",
        "print(\"\\n--- Preprocessing and Aggregating Reviews Data ---\")\n",
        "\n",
        "if not reviews_df.empty and 'id' in reviews_df.columns:\n",
        "    # Select relevant columns\n",
        "    reviews_cols = ['id', 'reviewText', 'scoreSentiment']\n",
        "    reviews_cols_exist = [col for col in reviews_cols if col in reviews_df.columns]\n",
        "    reviews_df_processed = reviews_df[reviews_cols_exist].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    if 'reviewText' in reviews_df_processed.columns:\n",
        "        reviews_df_processed.loc[:, 'reviewText'] = reviews_df_processed['reviewText'].fillna('')\n",
        "    else:\n",
        "        reviews_df_processed.loc[:, 'reviewText'] = '' # Create column if it doesn't exist\n",
        "\n",
        "    # Aggregate review text for each movie\n",
        "    if 'reviewText' in reviews_df_processed.columns and 'id' in reviews_df_processed.columns:\n",
        "        aggregated_reviews = reviews_df_processed.groupby('id')['reviewText'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "        aggregated_reviews.rename(columns={'reviewText': 'aggregatedReviewText'}, inplace=True)\n",
        "        print(\"\\nAggregated Reviews Head:\")\n",
        "        print(aggregated_reviews.head())\n",
        "    else:\n",
        "        print(\"Could not aggregate reviews: 'id' or 'reviewText' column missing in reviews_df_processed.\")\n",
        "        aggregated_reviews = pd.DataFrame({'id': [], 'aggregatedReviewText': []})\n",
        "\n",
        "else:\n",
        "    print(\"Reviews DataFrame is empty or 'id' column is missing. Skipping review aggregation.\")\n",
        "    aggregated_reviews = pd.DataFrame({'id': [], 'aggregatedReviewText': []})\n",
        "\n",
        "print(f\"\\nAggregated Reviews Shape: {aggregated_reviews.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWXsiziVcdrv",
        "outputId": "74627c35-6eb4-49fb-ec91-77ffca275fa2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Preprocessing and Aggregating Reviews Data ---\n",
            "\n",
            "Aggregated Reviews Head:\n",
            "              id                               aggregatedReviewText\n",
            "0       $5_a_day  $5 a Day isn't perfect, but it does examine so...\n",
            "1  009_re_cyborg  Despite its good looks, there's no escaping 00...\n",
            "2         00_mhz  ...a stylishly minimalist plot, executed beaut...\n",
            "3        0814255  The Lightning Thief is good juvenile entertain...\n",
            "4        0878835  Formless and complicated.  [Full review in Spa...\n",
            "\n",
            "Aggregated Reviews Shape: (69263, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Merge DataFrames\n",
        "print(\"\\n--- Merging DataFrames ---\")\n",
        "\n",
        "if not movies_df.empty and 'id' in movies_df.columns:\n",
        "    if not aggregated_reviews.empty and 'id' in aggregated_reviews.columns:\n",
        "        # Merge movies_df with aggregated_reviews\n",
        "        merged_df = pd.merge(movies_df, aggregated_reviews, on='id', how='left')\n",
        "        # Fill NaNs in 'aggregatedReviewText' that might result from movies without reviews\n",
        "        if 'aggregatedReviewText' in merged_df.columns:\n",
        "            merged_df['aggregatedReviewText'] = merged_df['aggregatedReviewText'].fillna('')\n",
        "        else:\n",
        "             merged_df['aggregatedReviewText'] = '' # Add column if merge didn't create it\n",
        "    else:\n",
        "        print(\"Aggregated reviews are empty or missing 'id'. Using only movies_df.\")\n",
        "        merged_df = movies_df.copy()\n",
        "        merged_df['aggregatedReviewText'] = '' # Add empty column if no reviews\n",
        "else:\n",
        "    print(\"Movies DataFrame is empty or missing 'id'. Cannot proceed with merging.\")\n",
        "    merged_df = pd.DataFrame() # Empty dataframe\n",
        "\n",
        "if not merged_df.empty:\n",
        "    print(\"\\nMerged DataFrame Head:\")\n",
        "    print(merged_df.head())\n",
        "    print(f\"\\nMerged DataFrame Shape: {merged_df.shape}\")\n",
        "else:\n",
        "    print(\"Merged DataFrame is empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FnRfmcgcmwP",
        "outputId": "6486d974-e1c5-45dc-ecac-5be70a247709"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merging DataFrames ---\n",
            "\n",
            "Merged DataFrame Head:\n",
            "                     id                title  audienceScore  tomatoMeter  \\\n",
            "0    space-zombie-bingo  Space Zombie Bingo!           50.0          0.0   \n",
            "1       the_green_grass      The Green Grass            0.0          0.0   \n",
            "2             love_lies           Love, Lies           43.0          0.0   \n",
            "3  the_sore_losers_1997          Sore Losers           60.0          0.0   \n",
            "4  dinosaur_island_2002      Dinosaur Island           70.0          0.0   \n",
            "\n",
            "  rating ratingContents                          genre  \\\n",
            "0    NaN                        Comedy, Horror, Sci-fi   \n",
            "1    NaN                                         Drama   \n",
            "2    NaN                                         Drama   \n",
            "3    NaN                    Action, Mystery & thriller   \n",
            "4    NaN                 Fantasy, Adventure, Animation   \n",
            "\n",
            "                        director                                  writer  \\\n",
            "0                  George Ormrod              George Ormrod,John Sabotta   \n",
            "1                Tiffany Edwards                         Tiffany Edwards   \n",
            "2  Park Heung-Sik,Heung-Sik Park  Ha Young-Joon,Jeon Yun-su,Song Hye-jin   \n",
            "3          John Michael McCarthy                   John Michael McCarthy   \n",
            "4                  Will Meugniot                                John Loy   \n",
            "\n",
            "                                aggregatedReviewText  \n",
            "0                                                     \n",
            "1                                                     \n",
            "2  Though let down by its routine love triangle n...  \n",
            "3                                                     \n",
            "4                                                     \n",
            "\n",
            "Merged DataFrame Shape: (143258, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Feature Engineering - Text Cleaning Function\n",
        "print(\"\\n--- Defining Text Cleaning Function ---\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"A simple function to clean text data.\"\"\"\n",
        "    if isinstance(text, list): # Handles cases where genre might be a list of strings\n",
        "        text = ' '.join(map(str, text)) # Ensure all elements are strings before joining\n",
        "    if not isinstance(text, str): # Ensure text is a string\n",
        "        text = str(text)\n",
        "    text = text.lower() # Lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Remove special characters (alphanumeric and spaces only)\n",
        "    return text.strip()\n",
        "\n",
        "print(\"clean_text function defined.\")\n",
        "# Test the function\n",
        "print(f\"Test clean_text('Action|Adventure!'): {clean_text('Action|Adventure!')}\")\n",
        "print(f\"Test clean_text(123): {clean_text(123)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKyOwW4hczBv",
        "outputId": "1d9da5b7-8899-445b-a1a9-4ad6e5a24606"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Defining Text Cleaning Function ---\n",
            "clean_text function defined.\n",
            "Test clean_text('Action|Adventure!'): actionadventure\n",
            "Test clean_text(123): 123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Feature Engineering - Apply Cleaning and Create Content Soup\n",
        "print(\"\\n--- Applying Cleaning and Creating Content Soup ---\")\n",
        "\n",
        "if not merged_df.empty:\n",
        "    # Apply cleaning to relevant text columns\n",
        "    text_cols_to_clean = ['title', 'genre', 'director', 'writer', 'ratingContents', 'aggregatedReviewText']\n",
        "    for col in text_cols_to_clean:\n",
        "        if col in merged_df.columns:\n",
        "             merged_df[col] = merged_df[col].apply(clean_text)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in merged_df for cleaning.\")\n",
        "\n",
        "    # Create the \"content soup\" - a single string combining relevant features\n",
        "    def create_soup(x):\n",
        "        # Ensure all components are strings and handle potential missing columns gracefully\n",
        "        genre = str(x.get('genre', '')) * 3 # Weight genre\n",
        "        director = str(x.get('director', '')) * 2 # Weight director\n",
        "        writer = str(x.get('writer', ''))\n",
        "        rating_contents = str(x.get('ratingContents', ''))\n",
        "        reviews = str(x.get('aggregatedReviewText', ''))\n",
        "        # title = str(x.get('title', '')) # Title can be added if desired for soup\n",
        "\n",
        "        return f\"{genre} {director} {writer} {rating_contents} {reviews}\".strip()\n",
        "\n",
        "    merged_df['soup'] = merged_df.apply(create_soup, axis=1)\n",
        "\n",
        "    print(\"\\nContent Soup Example (first few movies):\")\n",
        "    for i in range(min(3, len(merged_df))): # Show for first 3 or fewer if less data\n",
        "        title_display = merged_df['title'].iloc[i] if 'title' in merged_df.columns else 'N/A'\n",
        "        soup_display = merged_df['soup'].iloc[i][:200] if 'soup' in merged_df.columns else 'N/A'\n",
        "        print(f\"Movie: {title_display}\")\n",
        "        print(f\"Soup: {soup_display}...\")\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"Merged DataFrame is empty. Skipping feature engineering.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ORyIAWc5g4",
        "outputId": "20ea113f-1909-4b67-be58-5a0ccfeff021"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying Cleaning and Creating Content Soup ---\n",
            "\n",
            "Content Soup Example (first few movies):\n",
            "Movie: space zombie bingo\n",
            "Soup: comedy horror scificomedy horror scificomedy horror scifi george ormrodgeorge ormrod george ormrodjohn sabotta...\n",
            "--------------------\n",
            "Movie: the green grass\n",
            "Soup: dramadramadrama tiffany edwardstiffany edwards tiffany edwards...\n",
            "--------------------\n",
            "Movie: love lies\n",
            "Soup: dramadramadrama park heungsikheungsik parkpark heungsikheungsik park ha youngjoonjeon yunsusong hyejin  though let down by its routine love triangle narrative love lies has a lot going for it and is a...\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: TF-IDF Vectorization\n",
        "print(\"\\n--- TF-IDF Vectorization ---\")\n",
        "\n",
        "# Initialize tfidf_matrix to None outside the if blocks\n",
        "# This ensures the variable exists even if vectorization is skipped.\n",
        "tfidf_matrix = None\n",
        "\n",
        "if not merged_df.empty:\n",
        "    if 'soup' in merged_df.columns:\n",
        "        print(\"Processing 'soup' column for TF-IDF...\")\n",
        "        # Filter out rows where 'soup' is empty or NaN before vectorizing\n",
        "        soup_series = merged_df['soup'].dropna()\n",
        "        soup_series = soup_series[soup_series != '']\n",
        "\n",
        "        if not soup_series.empty:\n",
        "            print(f\"Found {len(soup_series)} non-empty 'soup' entries for TF-IDF.\")\n",
        "            tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=1) # min_df=1 to handle small vocab\n",
        "            try:\n",
        "                tfidf_matrix = tfidf.fit_transform(soup_series)\n",
        "                print(\"\\nTF-IDF Matrix Shape:\")\n",
        "                print(tfidf_matrix.shape) # (number of movies with non-empty soup, number of unique words/phrases)\n",
        "            except ValueError as e:\n",
        "                print(f\"Error during TF-IDF fitting: {e}\")\n",
        "                print(\"This might happen if the vocabulary is empty after processing (e.g., all soups are empty or only stop words).\")\n",
        "        else:\n",
        "            print(\"No non-empty 'soup' data available for TF-IDF after cleaning and filtering.\")\n",
        "    else:\n",
        "        print(\"Cannot perform TF-IDF: 'soup' column is missing in merged_df.\")\n",
        "else:\n",
        "    print(\"Cannot perform TF-IDF: merged_df is empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T06D7BvPdYRK",
        "outputId": "d8172d2e-2b7d-42ab-d005-11e1da23945f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TF-IDF Vectorization ---\n",
            "Processing 'soup' column for TF-IDF...\n",
            "Found 142815 non-empty 'soup' entries for TF-IDF.\n",
            "\n",
            "TF-IDF Matrix Shape:\n",
            "(142815, 9129733)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 (Modified): Cosine Similarity Calculation\n",
        "import numpy as np # Make sure numpy is imported if you use np.nan_to_num\n",
        "\n",
        "print(\"\\n--- Cosine Similarity Calculation ---\")\n",
        "\n",
        "cosine_sim = None # Initialize\n",
        "\n",
        "# Check if tfidf_matrix is valid and has both rows (movies) and columns (features)\n",
        "if tfidf_matrix is not None and tfidf_matrix.shape[0] > 0 and tfidf_matrix.shape[1] > 0:\n",
        "    try:\n",
        "        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "        # Optional: Handle potential NaN values if any document vectors were all zeros.\n",
        "        # Scikit-learn's cosine_similarity often handles this by returning 0 for similarity\n",
        "        # with a zero vector. If you encounter NaNs and they cause issues later (e.g., in sorting),\n",
        "        # you can convert them to zeros:\n",
        "        # cosine_sim = np.nan_to_num(cosine_sim) # Uncomment if needed\n",
        "\n",
        "        print(\"\\nCosine Similarity Matrix Shape:\")\n",
        "        print(cosine_sim.shape)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating cosine similarity: {e}\")\n",
        "elif tfidf_matrix is not None:\n",
        "    # This case handles when tfidf_matrix exists but is not suitable (e.g., no movies or no features)\n",
        "    print(f\"TF-IDF matrix is not suitable for similarity calculation. Shape: {tfidf_matrix.shape}. Skipping cosine similarity.\")\n",
        "    print(\"This can happen if there are no documents with valid 'soup' or if the vocabulary extracted is empty.\")\n",
        "else: # This case handles when tfidf_matrix itself is None\n",
        "    print(\"TF-IDF matrix is None (likely not created successfully in Cell 7). Skipping cosine similarity.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKX74xjzeJwS",
        "outputId": "ceb7a15b-0fa4-4db5-c7f8-1b89b4071666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cosine Similarity Calculation ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Recommendation Function Definition\n",
        "print(\"\\n--- Defining Recommendation Function ---\")\n",
        "\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Create a Series for reverse mapping of movie titles to indices\n",
        "# This mapping should be based on the movies that were actually included in the tfidf_matrix\n",
        "# (i.e., those with non-empty soup)\n",
        "indices_map = pd.Series(dtype='float64') # Initialize with dtype to avoid issues with empty Series\n",
        "\n",
        "if not merged_df.empty and 'title' in merged_df.columns and 'soup' in merged_df.columns:\n",
        "    # Filter merged_df to include only movies that contributed to tfidf_matrix\n",
        "    # These are movies with non-empty soup\n",
        "    df_for_indices = merged_df[merged_df['soup'].fillna('') != ''].copy()\n",
        "    df_for_indices.reset_index(drop=True, inplace=True) # Reset index to match tfidf_matrix rows\n",
        "\n",
        "    if not df_for_indices.empty:\n",
        "        # Ensure titles used for mapping are the cleaned ones from df_for_indices\n",
        "        # Use .iloc if title is not unique, though 'id' would be better for non-unique titles\n",
        "        if df_for_indices['title'].duplicated().any():\n",
        "            print(\"Warning: Duplicate movie titles exist in the filtered data. Using first occurrence for recommendations.\")\n",
        "        indices_map = pd.Series(df_for_indices.index, index=df_for_indices['title']).drop_duplicates()\n",
        "        print(f\"Indices map created with {len(indices_map)} entries.\")\n",
        "    else:\n",
        "        print(\"No data with non-empty soup to create indices map.\")\n",
        "else:\n",
        "    print(\"Merged DataFrame, 'title', or 'soup' column missing. Cannot create indices map.\")\n",
        "\n",
        "\n",
        "def get_recommendations(title, cosine_sim_matrix=cosine_sim, data=merged_df, idx_map=indices_map):\n",
        "    \"\"\"\n",
        "    Generates movie recommendations based on cosine similarity.\n",
        "    Args:\n",
        "        title (str): The title of the movie to get recommendations for.\n",
        "        cosine_sim_matrix (np.array): The precomputed cosine similarity matrix.\n",
        "        data (pd.DataFrame): The DataFrame containing original movie data (used for displaying results).\n",
        "                            It's assumed this 'data' df aligns with the original full movie list.\n",
        "        idx_map (pd.Series): A mapping from cleaned movie titles to their DataFrame indices\n",
        "                             (indices in the cosine_sim_matrix).\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of recommended movies with their scores.\n",
        "    \"\"\"\n",
        "    title_cleaned = clean_text(title)\n",
        "\n",
        "    if cosine_sim_matrix is None:\n",
        "        print(\"Error: Cosine similarity matrix is not available.\")\n",
        "        return pd.DataFrame()\n",
        "    if data.empty: # Check if the original data (merged_df) is empty\n",
        "        print(\"Error: Main data for recommendations (merged_df) is empty.\")\n",
        "        return pd.DataFrame()\n",
        "    if idx_map.empty:\n",
        "        print(\"Error: Indices map is empty.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if title_cleaned not in idx_map:\n",
        "        print(f\"Error: Movie '{title}' (cleaned: '{title_cleaned}') not found in the recommendation index.\")\n",
        "        possible_matches = [t for t in idx_map.index if title_cleaned in t or t in title_cleaned]\n",
        "        if possible_matches:\n",
        "            print(f\"Did you mean one of these? {possible_matches[:5]}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Get the index of the movie that matches the title in the TF-IDF matrix\n",
        "    matrix_idx = idx_map[title_cleaned]\n",
        "\n",
        "    # Get pairwise similarity scores of all movies with that movie\n",
        "    sim_scores = list(enumerate(cosine_sim_matrix[matrix_idx]))\n",
        "\n",
        "    # Sort movies based on similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get scores of the 10 most similar movies (excluding the movie itself, which will have score 1.0)\n",
        "    # sim_scores[0] will be the movie itself.\n",
        "    sim_scores = sim_scores[1:11]\n",
        "\n",
        "    # Get movie indices from the TF-IDF matrix perspective\n",
        "    tfidf_movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # recommendation_data_source should be the same df that was used to create the indices_map and tfidf_matrix\n",
        "    recommendation_data_source = merged_df[merged_df['soup'].fillna('') != ''].reset_index(drop=True)\n",
        "\n",
        "    # Columns to display in recommendations\n",
        "    display_cols = ['title', 'genre', 'director', 'audienceScore', 'tomatoMeter']\n",
        "    # Ensure these columns exist in the recommendation_data_source\n",
        "    actual_display_cols = [col for col in display_cols if col in recommendation_data_source.columns]\n",
        "\n",
        "    # Ensure we only try to iloc if tfidf_movie_indices are valid for recommendation_data_source\n",
        "    valid_indices = [idx for idx in tfidf_movie_indices if idx < len(recommendation_data_source)]\n",
        "    if not valid_indices:\n",
        "        print(\"No valid movie indices found for recommendations after filtering.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "    recommendations = recommendation_data_source[actual_display_cols].iloc[valid_indices].copy()\n",
        "    # Ensure similarity scores align with the potentially filtered valid_indices\n",
        "    recommendations['similarityScore'] = [s[1] for i, s in enumerate(sim_scores) if tfidf_movie_indices[i] in valid_indices]\n",
        "\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "print(\"get_recommendations function defined.\")"
      ],
      "metadata": {
        "id": "SndLDvMFs3yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Test Recommendations\n",
        "print(\"\\n--- Testing Recommendations ---\")\n",
        "\n",
        "if cosine_sim is not None and not merged_df.empty and not indices_map.empty:\n",
        "    # Try to find a movie that exists in the indices_map for testing\n",
        "    test_movie_title = \"\"\n",
        "    if not indices_map.empty:\n",
        "        test_movie_title = indices_map.index[0] # Get the first movie title from the map\n",
        "        print(f\"\\nAttempting to get recommendations for: '{test_movie_title}' (using cleaned version from dataset)\")\n",
        "\n",
        "        recommendations = get_recommendations(test_movie_title, cosine_sim_matrix=cosine_sim, data=merged_df, idx_map=indices_map)\n",
        "\n",
        "        if not recommendations.empty:\n",
        "            print(recommendations)\n",
        "        else:\n",
        "            print(f\"Could not get recommendations for '{test_movie_title}'. Check if it's in 'indices_map' and if 'soup' was generated.\")\n",
        "    else:\n",
        "        print(\"No titles available in `indices_map` for testing.\")\n",
        "\n",
        "    print(\"\\n--- Try another example (you might need to change the title) ---\")\n",
        "    # For dummy data, let's try 'Dummy Movie 2' if it exists in indices_map\n",
        "    another_test_title = \"Dummy Movie 2\" # Example, change if not in your data (or 'Another Dummy Film')\n",
        "\n",
        "    # Check if the title (after cleaning) is in the indices_map\n",
        "    if clean_text(another_test_title) in indices_map:\n",
        "        print(f\"Attempting to get recommendations for: '{another_test_title}'\")\n",
        "        recommendations_dk = get_recommendations(another_test_title, cosine_sim_matrix=cosine_sim, data=merged_df, idx_map=indices_map)\n",
        "        if not recommendations_dk.empty:\n",
        "            print(recommendations_dk)\n",
        "        else:\n",
        "            print(f\"Could not get recommendations for '{another_test_title}'.\")\n",
        "    else:\n",
        "        print(f\"'{another_test_title}' (cleaned: '{clean_text(another_test_title)}') not in indices_map. Try a title from your actual dataset or one of these from the dummy set (if available): {list(indices_map.index[:3])}\")\n",
        "\n",
        "else:\n",
        "    print(\"Cannot test recommendations: cosine similarity matrix, merged_df, or indices_map is not available/empty.\")\n",
        "    if cosine_sim is None:\n",
        "        print(\"Reason: cosine_sim is None.\")\n",
        "    if merged_df.empty:\n",
        "        print(\"Reason: merged_df is empty.\")\n",
        "    if 'indices_map' not in globals() or indices_map.empty :\n",
        "         print(\"Reason: indices_map is not defined or empty.\")"
      ],
      "metadata": {
        "id": "a6D7ca8wtXdw",
        "outputId": "d2043bd0-1817-48aa-fd10-46f2a86914c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Recommendations ---\n",
            "Cannot test recommendations: cosine similarity matrix, merged_df, or indices_map is not available/empty.\n",
            "Reason: cosine_sim is None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'merged_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4183e5795c92>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcosine_sim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reason: cosine_sim is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reason: merged_df is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'indices_map'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindices_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'merged_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Further Potential Enhancements (Informational)\n",
        "\n",
        "print(\"\\n--- Further Potential Enhancements ---\")\n",
        "print(\"1. More Sophisticated Text Cleaning: Lemmatization, stemming, handling specific phrases.\")\n",
        "print(\"2. Advanced NLP: Use Word Embeddings (Word2Vec, GloVe, Sentence Transformers) instead of TF-IDF for richer semantic understanding.\")\n",
        "print(\"3. Incorporate More Features: Use numerical features like 'audienceScore' and 'tomatoMeter' directly in the similarity calculation (after scaling), or use them to re-rank TF-IDF based recommendations.\")\n",
        "print(\"4. Handling Duplicates & IDs: More robust handling of duplicate movie titles by primarily using movie 'id' for mapping and lookups, rather than 'title'. The current 'indices_map' uses titles which might not be unique.\")\n",
        "print(\"5. User Interface: Build a simple UI (e.g., using Streamlit or Flask) to interact with the recommender.\")\n",
        "print(\"6. Evaluation: Implement methods to evaluate the quality of recommendations (this is complex for content-based systems without explicit user feedback).\")\n",
        "print(\"7. Scalability: For very large datasets, explore approximate nearest neighbor algorithms (e.g., Annoy, Faiss) instead of exact cosine similarity.\")\n",
        "print(\"8. Parameter Tuning: Experiment with TF-IDF parameters (min_df, max_df, ngram_range) and soup creation weights.\")\n"
      ],
      "metadata": {
        "id": "M8MMAcRRtmGI",
        "outputId": "7e1ef2a4-cdd1-4fff-cb1d-cc0c201d60ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Further Potential Enhancements ---\n",
            "1. More Sophisticated Text Cleaning: Lemmatization, stemming, handling specific phrases.\n",
            "2. Advanced NLP: Use Word Embeddings (Word2Vec, GloVe, Sentence Transformers) instead of TF-IDF for richer semantic understanding.\n",
            "3. Incorporate More Features: Use numerical features like 'audienceScore' and 'tomatoMeter' directly in the similarity calculation (after scaling), or use them to re-rank TF-IDF based recommendations.\n",
            "4. Handling Duplicates & IDs: More robust handling of duplicate movie titles by primarily using movie 'id' for mapping and lookups, rather than 'title'. The current 'indices_map' uses titles which might not be unique.\n",
            "5. User Interface: Build a simple UI (e.g., using Streamlit or Flask) to interact with the recommender.\n",
            "6. Evaluation: Implement methods to evaluate the quality of recommendations (this is complex for content-based systems without explicit user feedback).\n",
            "7. Scalability: For very large datasets, explore approximate nearest neighbor algorithms (e.g., Annoy, Faiss) instead of exact cosine similarity.\n",
            "8. Parameter Tuning: Experiment with TF-IDF parameters (min_df, max_df, ngram_range) and soup creation weights.\n"
          ]
        }
      ]
    }
  ]
}