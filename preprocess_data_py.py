# -*- coding: utf-8 -*-
"""preprocess_data.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mL_rhvRSEGyfI5sh4dFtnw9uk3OUJxBV
"""

# preprocess_data.py
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import joblib
import os
import re

def clean_text(text):
    """Utility function to clean text by removing special characters and converting to lowercase."""
    if isinstance(text, str):
        text = re.sub(r'[^\w\s]', '', text) # Remove punctuation
        text = text.lower() # Convert to lowercase
        return text
    return ""

def create_content_soup(df_row):
    """Combines relevant movie features into a single string for TF-IDF."""
    soup_parts = [
        clean_text(df_row.get('genres', '')),
        clean_text(df_row.get('movie_info', '')), # Synopsis
        clean_text(df_row.get('directors', '')),
        clean_text(df_row.get('actors', '')),
        clean_text(df_row.get('critics_consensus', '')), # Could be useful
        clean_text(df_row.get('content_rating', ''))
    ]
    # Add individual keywords if available, e.g., from a 'keywords' column if you had one
    # For now, we rely on the text fields above.
    return ' '.join(filter(None, soup_parts)).strip()

def main():
    print("Step 1: Loading data...")
    data_dir = 'data'
    movies_file = os.path.join(data_dir, 'rotten_tomatoes_movies.csv')
    reviews_file = os.path.join(data_dir, 'rotten_tomatoes_movie_reviews-1.csv') # Not directly used for content features, but good to acknowledge

    if not os.path.exists(movies_file):
        print(f"Error: Movies file not found at {movies_file}")
        print("Please ensure 'rotten_tomatoes_movies.csv' is in the 'data' subfolder.")
        return

    # For this recommender, we primarily need the movies metadata.
    # Reviews could be used for collaborative filtering or sentiment-based features,
    # but for content-based, movies_df is key.
    movies_df_full = pd.read_csv(movies_file)

    print(f"Initial movies_df_full shape: {movies_df_full.shape}")

    # --- Data Cleaning and Selection ---
    # Select relevant columns for content-based filtering
    # Adjust these columns based on what you found most useful in your notebook
    relevant_cols = [
        'movie_title', 'movie_info', 'genres', 'directors', 'actors',
        'critics_consensus', 'content_rating', 'rotten_tomatoes_link' # Keep link for reference if needed
    ]
    # Filter out columns that might not exist to prevent errors
    existing_cols = [col for col in relevant_cols if col in movies_df_full.columns]
    movies_df = movies_df_full[existing_cols].copy()

    # Rename for consistency if needed (e.g., 'movie_title' to 'title')
    if 'movie_title' in movies_df.columns:
        movies_df.rename(columns={'movie_title': 'title'}, inplace=True)
    else:
        print("Error: 'movie_title' column not found. Please check your CSV.")
        return

    print(f"Movies_df shape after selecting columns: {movies_df.shape}")

    # --- Handle Missing Values ---
    # For text features, fill NaN with empty strings so they don't cause errors
    text_feature_cols = ['movie_info', 'genres', 'directors', 'actors', 'critics_consensus', 'content_rating']
    for col in text_feature_cols:
        if col in movies_df.columns:
            movies_df[col] = movies_df[col].fillna('')
        else:
            movies_df[col] = '' # Add empty column if it was missing but expected

    # --- Remove Duplicates ---
    # Keep the first occurrence if duplicates exist based on title
    movies_df.drop_duplicates(subset=['title'], keep='first', inplace=True)
    print(f"Movies_df shape after dropping duplicates: {movies_df.shape}")

    # --- Feature Engineering: Create 'content_soup' ---
    print("\nStep 2: Creating content soup for each movie...")
    movies_df['content_soup'] = movies_df.apply(create_content_soup, axis=1)

    # Filter out movies with empty soup if any (should be rare if title is present)
    movies_df = movies_df[movies_df['content_soup'] != '']
    print(f"Movies_df shape after creating and filtering soup: {movies_df.shape}")

    if movies_df.empty:
        print("Error: No data left after preprocessing. Check input data and cleaning steps.")
        return

    # --- TF-IDF Vectorization ---
    print("\nStep 3: Performing TF-IDF Vectorization...")
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=2, max_df=0.7)
    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['content_soup'])
    print(f"TF-IDF matrix shape: {tfidf_matrix.shape}")

    # --- Cosine Similarity ---
    print("\nStep 4: Calculating Cosine Similarity Matrix...")
    # For large datasets, consider alternatives or chunking if memory becomes an issue.
    # sklearn.metrics.pairwise.linear_kernel is often faster than cosine_similarity for TF-IDF.
    # cosine_sim_matrix = linear_kernel(tfidf_matrix, tfidf_matrix)
    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
    print(f"Cosine similarity matrix shape: {cosine_sim_matrix.shape}")

    # --- Prepare data for serving ---
    # We need a DataFrame that's indexed from 0 to N-1 to easily map similarity scores back to titles.
    # The 'title' column is essential.
    movies_df_indexed = movies_df[['title']].copy() # Keep only title, or other info you want to display
    movies_df_indexed.reset_index(drop=True, inplace=True) # Ensure 0-based integer index

    # Create a mapping from movie title to its index in movies_df_indexed
    # This is crucial for quickly finding a movie's row in the cosine_sim_matrix
    title_to_indices = pd.Series(movies_df_indexed.index, index=movies_df_indexed['title'])
    # Handle potential duplicate titles if any slipped through (though drop_duplicates should prevent this)
    # If duplicates exist, title_to_indices might map a title to multiple indices.
    # For simplicity, we assume titles are unique after drop_duplicates.
    # If not, you might need a more robust way to handle this, e.g., take the first index.
    # title_to_indices = title_to_indices[~title_to_indices.index.duplicated(keep='first')]


    # --- Save Artifacts ---
    print("\nStep 5: Saving artifacts...")
    # We don't strictly need to save the tfidf_vectorizer if we are not vectorizing new, unseen text at runtime.
    # For this content-based recommender based on existing items, cosine_sim_matrix and the indexed movie list are key.
    # joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib') # Optional
    np.save('cosine_sim_matrix.npy', cosine_sim_matrix)
    movies_df_indexed.to_parquet('movies_df_indexed.parquet') # Parquet is efficient for DataFrames
    joblib.dump(title_to_indices, 'title_to_indices.joblib')

    print("\nPreprocessing complete. Artifacts saved:")
    print("- cosine_sim_matrix.npy")
    print("- movies_df_indexed.parquet")
    print("- title_to_indices.joblib")

if __name__ == '__main__':
    main()